#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
@File ï¼šingest.py
@Author ï¼šzqy
@Email : zqingy@work@163.com
@note: æ–‡æ¡£åˆ‡å— + å…¥åº“ + å»ºç´¢å¼•ï¼ˆingest.pyï¼‰
"""
import re
import time
from pathlib import Path
from typing import Iterable, List, Tuple

from app.agent.rag.db import get_conn, init_rag_tables

KB_DIR = Path("data/kb")
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
BATCH_SIZE = 200  # æ‰¹é‡å†™å…¥æ›´å¤§ä¸€äº›æ›´å¿«ï¼ˆå¯æŒ‰éœ€è°ƒï¼‰

CHUNK_MAX_CHARS = 700
CHUNK_OVERLAP = 80


def log(msg: str):
    """å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è¾“å‡º"""
    print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {msg}")


def _normalize_text(s: str) -> str:
    # æŠŠè¿‡å¤šç©ºè¡Œå‹ç¼©ä¸€ä¸‹ï¼Œé¡ºä¾¿å»æ‰é¦–å°¾ç©ºç™½
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def _iter_chunks_from_text(text: str, max_chars: int = CHUNK_MAX_CHARS, overlap: int = CHUNK_OVERLAP) -> List[str]:
    """
    çº¯æ–‡æœ¬åˆ‡å—ï¼ˆä¿®å¤æ­»å¾ªç¯ç‰ˆæœ¬ï¼‰
    """
    if not text:
        return []
    if overlap >= max_chars:
        raise ValueError(f"overlap({overlap}) must be < max_chars({max_chars})")

    chunks: List[str] = []
    i = 0
    n = len(text)

    while i < n:
        end = min(i + max_chars, n)
        chunk = text[i:end].strip()
        if chunk:
            chunks.append(chunk)

        if end == n:  # âœ… åˆ°æœ«å°¾äº†å¿…é¡»é€€å‡ºï¼Œå¦åˆ™ i å¯èƒ½å¡ä½
            break

        next_i = end - overlap
        if next_i <= i:  # âœ… åŒä¿é™©é˜²æ­¢å¡ä½
            break
        i = next_i

    return chunks


def _iter_chunks_streaming(path: Path, max_chars: int = CHUNK_MAX_CHARS, overlap: int = CHUNK_OVERLAP) -> Iterable[str]:
    """
    çœŸæ­£æµå¼åˆ‡å—ï¼šè¾¹è¯»æ–‡ä»¶è¾¹ç”Ÿæˆ chunkï¼Œä¸æŠŠå…¨æ–‡ä¸€æ¬¡æ€§åŠ è½½è¿›å†…å­˜ã€‚
    é€»è¾‘ï¼šç»´æŠ¤ä¸€ä¸ªç¼“å†²åŒº bufferï¼Œå¤Ÿé•¿å°±åå‡º chunkï¼Œå¹¶ä¿ç•™ overlap éƒ¨åˆ†ã€‚
    """
    if overlap >= max_chars:
        raise ValueError(f"overlap({overlap}) must be < max_chars({max_chars})")

    file_size = path.stat().st_size
    if file_size > MAX_FILE_SIZE:
        log(f"âš ï¸ è·³è¿‡è¶…å¤§æ–‡ä»¶: {path} ({file_size / 1024 / 1024:.1f} MB)")
        return

    log(f"ğŸ“„ å¼€å§‹è¯»å–æ–‡ä»¶: {path} ({file_size / 1024:.1f} KB)")

    buffer = ""
    produced = 0

    with path.open("r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            buffer += line

            # æ§åˆ¶ buffer çš„ç©ºè¡Œå¯†åº¦ï¼ˆå¯é€‰ï¼Œä½†èƒ½é˜²æ­¢ç©ºè¡Œçˆ†ç‚¸ï¼‰
            if "\n\n\n" in buffer:
                buffer = re.sub(r"\n{3,}", "\n\n", buffer)

            while len(buffer) >= max_chars:
                chunk = buffer[:max_chars].strip()
                if chunk:
                    produced += 1
                    yield chunk

                # ä¿ç•™ overlap éƒ¨åˆ†
                buffer = buffer[max_chars - overlap :]

    # æ–‡ä»¶è¯»å®Œï¼Œå¤„ç†æœ€åå‰©ä½™å†…å®¹
    buffer = _normalize_text(buffer)
    if buffer:
        # æœ€åä¸€æ®µå¯èƒ½ä»ç„¶å¾ˆé•¿ï¼Œå¤ç”¨éæµå¼åˆ‡å—ï¼ˆå·²ä¿®å¤æ­»å¾ªç¯ï¼‰
        tail_chunks = _iter_chunks_from_text(buffer, max_chars=max_chars, overlap=overlap)
        for c in tail_chunks:
            produced += 1
            yield c

    log(f"âœ… è¯»å–å®Œæˆ: {path} (ç”Ÿæˆ chunk æ•°: {produced})")


def reindex_kb() -> None:
    """æŠŠ data/kb ä¸‹æ‰€æœ‰ .md/.txt é‡æ–°å…¥åº“å¹¶é‡å»ºç´¢å¼•"""
    log("ğŸš€ å¼€å§‹é‡å»º KB ç´¢å¼•")

    init_rag_tables()
    conn = get_conn()
    cur = conn.cursor()

    try:
        # 1) æ¸…ç©ºæ—§ç´¢å¼•
        log("ğŸ—‘ï¸ æ¸…ç©ºæ—§æ•°æ®...")
        cur.execute("DELETE FROM kb_chunk_fts")
        cur.execute("DELETE FROM kb_chunk")
        cur.execute("DELETE FROM kb_doc")
        conn.commit()
        log("âœ… æ—§æ•°æ®å·²æ¸…ç©º")

        # 2) éå†æ–‡ä»¶å…¥åº“
        log("ğŸ“‚ å¼€å§‹éå†æ–‡ä»¶ç›®å½•...")
        paths = sorted([p for p in KB_DIR.glob("**/*") if p.suffix.lower() in {".md", ".txt"}])
        log(f"ğŸ“Š å…±æ‰¾åˆ° {len(paths)} ä¸ªæ–‡æ¡£")

        total_chunks = 0

        for idx, p in enumerate(paths, start=1):
            log(f"\n[{idx}/{len(paths)}] å¤„ç†æ–‡ä»¶: {p}")

            title = p.stem
            cur.execute("INSERT INTO kb_doc(path, title) VALUES (?, ?)", (p.as_posix(), title))
            doc_id = cur.lastrowid
            log(f"ğŸ“ æ’å…¥ kb_doc: {title} (doc_id={doc_id})")

            # æ‰¹é‡ç¼“å­˜
            pending_chunk_rows: List[Tuple[int, int, str]] = []
            pending_fts_rows: List[Tuple[int, str]] = []

            chunk_index = 0

            for chunk_text in _iter_chunks_streaming(p):
                # å…ˆæ’å…¥ kb_chunk ä»¥æ‹¿åˆ° rowid(chunk_id)ï¼Œfts ç”¨ rowid å¯¹é½
                cur.execute(
                    "INSERT INTO kb_chunk(doc_id, chunk_index, content) VALUES (?, ?, ?)",
                    (doc_id, chunk_index, chunk_text),
                )
                chunk_id = cur.lastrowid
                cur.execute(
                    "INSERT INTO kb_chunk_fts(rowid, content) VALUES (?, ?)",
                    (chunk_id, chunk_text),
                )

                chunk_index += 1
                total_chunks += 1

                if total_chunks % BATCH_SIZE == 0:
                    conn.commit()
                    log(f"ğŸ’¾ æ‰¹é‡æäº¤: å·²å¤„ç† {total_chunks} å—")

            conn.commit()
            log(f"âœ… æ–‡æ¡£å®Œæˆ: {p} (chunks={chunk_index})")

        log(f"\nğŸ‰ ç´¢å¼•é‡å»ºå®Œæˆï¼å…±å¤„ç† {total_chunks} å—æ•°æ®")

    except Exception as e:
        conn.rollback()
        log(f"âŒ ç´¢å¼•é‡å»ºå¤±è´¥ï¼Œå·²å›æ»šã€‚é”™è¯¯: {e}")
        raise
    finally:
        conn.close()


if __name__ == "__main__":
    reindex_kb()
    print("KB reindexed âœ…")
